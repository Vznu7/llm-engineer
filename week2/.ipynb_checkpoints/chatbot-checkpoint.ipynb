{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e626c2dd-c042-4a34-8973-8b3bd963a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15301d5b-8003-4168-bf15-772db3ec92f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key not set\n",
      "Google API Key exists and begins AIzaSyA0\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00b02cd3-c400-4774-8ecf-8849a0fe0d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai= OpenAI()\n",
    "google.generativeai.configure()\n",
    "MODEL ='gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99f853c1-e1d8-46ea-afd1-c55d10f033c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = 'you are a helpful assistant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef732ae2-50d3-42af-85cb-3a1b5e7c8f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gpt(message,history):\n",
    "    messages=[{\"role\":\"system\",\"content\":System_message}]+history+[{\"role\":\"user\",\"content\":message}]\n",
    "\n",
    "    stream =openai.chat.completions.create(model=MODEL,messages=messages,stream=True)\n",
    "\n",
    "\n",
    "    response =\"\"\n",
    "    for chunk in stream:\n",
    "        response+=chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bc94787-9bee-4a26-a9d5-846cbaec714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gemini(message,history):\n",
    "    gemini =google.generativeai.GenerativeModel(\n",
    "        model_name ='gemini-2.0-flash',\n",
    "        system_instruction =system_message\n",
    "    )\n",
    "    messages = history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response =gemini.generate_content(messages,stream=True)\n",
    "\n",
    "\n",
    "    result =\"\"\n",
    "    for chunk in response:\n",
    "        if chunk.candidates and chunk.candidates[0].content.parts:\n",
    "            text =chunk.candidates[0].content.parts[0].text\n",
    "            result+=text\n",
    "            yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3bc5539-7f4f-49f5-a021-1f350d3e2a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7877\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7877/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 745, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 353, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1635, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 760, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 865, in asyncgen_wrapper\n",
      "    response = await iterator.__anext__()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\chat_interface.py\", line 546, in _wrapper\n",
      "    async for chunk in submit_fn(*args, **kwargs):\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\chat_interface.py\", line 978, in _stream_fn\n",
      "    first_response = await utils.async_iteration(generator)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 760, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 751, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 734, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vishn\\AppData\\Local\\Temp\\ipykernel_8856\\921174721.py\", line 4, in chat\n",
      "    yield from chat_gpt(message, history)\n",
      "  File \"C:\\Users\\vishn\\AppData\\Local\\Temp\\ipykernel_8856\\610235039.py\", line 2, in chat_gpt\n",
      "    messages=[{\"role\":\"system\",\"content\":System_message}]+history+[{\"role\":\"user\",\"content\":message}]\n",
      "                                         ^^^^^^^^^^^^^^\n",
      "NameError: name 'System_message' is not defined\n"
     ]
    }
   ],
   "source": [
    "def chat(message, history, model):\n",
    "    \"\"\"Main chat function for gr.ChatInterface\"\"\"\n",
    "    if model == \"GPT-4o-mini\":\n",
    "        yield from chat_gpt(message, history)\n",
    "    elif model == \"Gemini 2.0 Flash\":\n",
    "        yield from chat_gemini(message, history)\n",
    "    else:\n",
    "        yield \"Please select a model\"\n",
    "\n",
    "\n",
    "# ChatInterface with model selection dropdown\n",
    "gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    type='messages',\n",
    "    title=\"Multi-Model Chat\",\n",
    "    description=\"Chat with GPT or Gemini\",\n",
    "    additional_inputs=[\n",
    "        gr.Dropdown(\n",
    "            choices=[\"GPT-4o-mini\", \"Gemini 2.0 Flash\"],\n",
    "            value=\"GPT-4o-mini\",\n",
    "            label=\"Select Model\"\n",
    "        )\n",
    "    ]\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12de4d20-da41-4a33-8c76-5497950eb8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_model(message,model):\n",
    "    if model ==\"GPT\":\n",
    "        result= chat_gpt(message)\n",
    "    elif model ==\"gemini\":\n",
    "        result =chat_gemini(message)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d1f05-ce5c-48f7-9243-5dae1ef36f54",
   "metadata": {},
   "source": [
    "### Gradio integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29ccc9c8-b87a-421c-a3fd-cd51ac27b1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7874\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat,type='messages').launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afc97cf2-c7a9-44a9-8717-a1498b1874a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ChatInterface.__init__() got an unexpected keyword argument 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m view =\u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChatInterface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43myour message\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDropdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGPT\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mselect model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGPT\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMarkdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mResponse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflagging_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnever\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m view.launch()\n",
      "\u001b[31mTypeError\u001b[39m: ChatInterface.__init__() got an unexpected keyword argument 'inputs'"
     ]
    }
   ],
   "source": [
    "view =gr.ChatInterface(\n",
    "    fn=stream_model,\n",
    "    inputs=[gr.Textbox(label=\"your message\"),gr.Dropdown([\"GPT\",\"gemini\"],label=\"select model\",value=\"GPT\")],\n",
    "    outputs =[gr.Markdown(label=\"Response\")],\n",
    "    flagging_mode =\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e9bdb-e6e8-4299-ac0d-4ca2726aedbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
