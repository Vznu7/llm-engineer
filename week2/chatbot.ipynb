{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e626c2dd-c042-4a34-8973-8b3bd963a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15301d5b-8003-4168-bf15-772db3ec92f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key not set\n",
      "Google API Key exists and begins AIzaSyA0\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00b02cd3-c400-4774-8ecf-8849a0fe0d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai= OpenAI()\n",
    "google.generativeai.configure()\n",
    "MODEL ='gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99f853c1-e1d8-46ea-afd1-c55d10f033c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = 'you are a helpful assistant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef732ae2-50d3-42af-85cb-3a1b5e7c8f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gpt(message,history):\n",
    "    messages=[{\"role\":\"system\",\"content\":system_message}]+history+[{\"role\":\"user\",\"content\":message}]\n",
    "\n",
    "    stream =openai.chat.completions.create(model=MODEL,messages=messages,stream=True)\n",
    "\n",
    "\n",
    "    response =\"\"\n",
    "    for chunk in stream:\n",
    "        response+=chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bc94787-9bee-4a26-a9d5-846cbaec714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gemini(message, history):\n",
    "    \"\"\"Chat with Gemini model using messages format\"\"\"\n",
    "    gemini = google.generativeai.GenerativeModel(\n",
    "        model_name='gemini-2.0-flash-exp',\n",
    "        system_instruction=system_message\n",
    "    )\n",
    "    \n",
    "    # Convert history to Gemini format\n",
    "    # Extract only 'role' and 'content' from history messages\n",
    "    messages = []\n",
    "    for msg in history:\n",
    "        role = msg.get(\"role\", \"\")\n",
    "        content = msg.get(\"content\", \"\")\n",
    "        \n",
    "        if role == \"user\":\n",
    "            messages.append({\"role\": \"user\", \"parts\": [content]})\n",
    "        elif role == \"assistant\":\n",
    "            messages.append({\"role\": \"model\", \"parts\": [content]})\n",
    "    \n",
    "    # Add the current user message\n",
    "    messages.append({\"role\": \"user\", \"parts\": [message]})\n",
    "    \n",
    "    response = gemini.generate_content(messages, stream=True)\n",
    "    result = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.candidates and chunk.candidates[0].content.parts:\n",
    "            text = chunk.candidates[0].content.parts[0].text\n",
    "            result += text\n",
    "            yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23c1b2d4-2bc0-4625-9d1d-d4a29e11fcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7887\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7887/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat(message, history, model):\n",
    "    \"\"\"Main chat function for gr.ChatInterface\"\"\"\n",
    "    if model == \"GPT-4o-mini\":\n",
    "        yield from chat_gpt(message, history)\n",
    "    elif model == \"Gemini 2.0 Flash\":\n",
    "        yield from chat_gemini(message, history)\n",
    "    else:\n",
    "        yield \"Please select a model\"\n",
    "\n",
    "\n",
    "# Using Blocks for custom accordion label\n",
    "with gr.Blocks(title=\"Multi-Model Chat\") as demo:\n",
    "    gr.Markdown(\"# Multi-Model Chat\")\n",
    "    gr.Markdown(\"WHERE SHOULD WE BEGIN ?\")\n",
    "    \n",
    "    chatbot = gr.Chatbot(type='messages', label=\"Chat with me üíï\")\n",
    "    \n",
    "    with gr.Accordion(\"MODEL\", open=False):\n",
    "        model_dropdown = gr.Dropdown(\n",
    "            choices=[\"GPT-4o-mini\", \"Gemini 2.0 Flash\"],\n",
    "            value=\"GPT-4o-mini\",\n",
    "            label=\"Select Model\",\n",
    "            container=False\n",
    "        )\n",
    "    \n",
    "    msg = gr.Textbox(\n",
    "        placeholder=\"Type a message...\",\n",
    "        container=False,\n",
    "        scale=7\n",
    "    )\n",
    "    \n",
    "    def respond(message, chat_history, model):\n",
    "        # Add user message to history\n",
    "        chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "        \n",
    "        # Stream bot response\n",
    "        bot_response = \"\"\n",
    "        for response in chat(message, chat_history[:-1], model):\n",
    "            bot_response = response\n",
    "            # Update chat history with streaming response\n",
    "            updated_history = chat_history[:-1] + [\n",
    "                {\"role\": \"user\", \"content\": message},\n",
    "                {\"role\": \"assistant\", \"content\": bot_response}\n",
    "            ]\n",
    "            yield \"\", updated_history\n",
    "    \n",
    "    msg.submit(respond, [msg, chatbot, model_dropdown], [msg, chatbot])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd6387a2-a0d9-4716-af81-56e136da044d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7884\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7884/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 101, in handle_request\n",
      "    raise exc\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 78, in handle_request\n",
      "    stream = self._connect(request)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 124, in _connect\n",
      "    stream = self._network_backend.connect_tcp(**kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpcore\\_backends\\sync.py\", line 207, in connect_tcp\n",
      "    with map_exceptions(exc_map):\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.ConnectError: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\openai\\_base_client.py\", line 982, in request\n",
      "    response = self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpx\\_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpx\\_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpx\\_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpx\\_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 249, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.ConnectError: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 745, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 353, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vishn\\AppData\\Local\\Temp\\ipykernel_8856\\77958889.py\", line 35, in respond\n",
      "    for response in chat(message, chat_history[:-1], model):\n",
      "  File \"C:\\Users\\vishn\\AppData\\Local\\Temp\\ipykernel_8856\\77958889.py\", line 4, in chat\n",
      "    yield from chat_gpt(message, history)\n",
      "  File \"C:\\Users\\vishn\\AppData\\Local\\Temp\\ipykernel_8856\\2935006672.py\", line 4, in chat_gpt\n",
      "    stream =openai.chat.completions.create(model=MODEL,messages=messages,stream=True)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 286, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\openai\\_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\openai\\_base_client.py\", line 1014, in request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "openai.APIConnectionError: Connection error.\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 101, in handle_request\n",
      "    raise exc\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 78, in handle_request\n",
      "    stream = self._connect(request)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 124, in _connect\n",
      "    stream = self._network_backend.connect_tcp(**kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpcore\\_backends\\sync.py\", line 207, in connect_tcp\n",
      "    with map_exceptions(exc_map):\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.ConnectError: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\openai\\_base_client.py\", line 982, in request\n",
      "    response = self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpx\\_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpx\\_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpx\\_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpx\\_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 249, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.ConnectError: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 745, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 353, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vishn\\AppData\\Local\\Temp\\ipykernel_8856\\77958889.py\", line 35, in respond\n",
      "    for response in chat(message, chat_history[:-1], model):\n",
      "  File \"C:\\Users\\vishn\\AppData\\Local\\Temp\\ipykernel_8856\\77958889.py\", line 4, in chat\n",
      "    yield from chat_gpt(message, history)\n",
      "  File \"C:\\Users\\vishn\\AppData\\Local\\Temp\\ipykernel_8856\\2935006672.py\", line 4, in chat_gpt\n",
      "    stream =openai.chat.completions.create(model=MODEL,messages=messages,stream=True)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 286, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\openai\\_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\gen-ai\\anaconda\\envs\\llms\\Lib\\site-packages\\openai\\_base_client.py\", line 1014, in request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "openai.APIConnectionError: Connection error.\n"
     ]
    }
   ],
   "source": [
    "def chat(message, history, model):\n",
    "    \"\"\"Main chat function for gr.ChatInterface\"\"\"\n",
    "    if model == \"GPT-4o-mini\":\n",
    "        yield from chat_gpt(message, history)\n",
    "    elif model == \"Gemini 2.0 Flash\":\n",
    "        yield from chat_gemini(message, history)\n",
    "    else:\n",
    "        yield \"Please select a model\"\n",
    "\n",
    "\n",
    "# Using Blocks for custom accordion label\n",
    "with gr.Blocks(title=\"Multi-Model Chat\") as demo:\n",
    "    gr.Markdown(\"# Multi-Model Chat\")\n",
    "    gr.Markdown(\"WHERE SHOULD WE BEGIN ?\")\n",
    "    \n",
    "    chatbot = gr.Chatbot(type='messages', label=\"Chat with me üíï\")\n",
    "    \n",
    "    with gr.Accordion(\"MODEL\", open=False):\n",
    "        model_dropdown = gr.Dropdown(\n",
    "            choices=[\"GPT-4o-mini\", \"Gemini 2.0 Flash\"],\n",
    "            value=\"GPT-4o-mini\",\n",
    "            label=\"Select Model\",\n",
    "            container=False\n",
    "        )\n",
    "    \n",
    "    msg = gr.Textbox(\n",
    "        placeholder=\"Type a message...\",\n",
    "        container=False,\n",
    "        scale=7\n",
    "    )\n",
    "    \n",
    "    def respond(message, chat_history, model):\n",
    "        chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "        bot_response = \"\"\n",
    "        for response in chat(message, chat_history[:-1], model):\n",
    "            bot_response = response\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
    "        return \"\", chat_history\n",
    "    \n",
    "    msg.submit(respond, [msg, chatbot, model_dropdown], [msg, chatbot])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3bc5539-7f4f-49f5-a021-1f350d3e2a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7881\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7881/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat(message, history, model):\n",
    "    \"\"\"Main chat function for gr.ChatInterface\"\"\"\n",
    "    if model == \"GPT-4o-mini\":\n",
    "        yield from chat_gpt(message, history)\n",
    "    elif model == \"Gemini 2.0 Flash\":\n",
    "        yield from chat_gemini(message, history)\n",
    "    else:\n",
    "        yield \"Please select a model\"\n",
    "\n",
    "\n",
    "# ChatInterface with model selection dropdown\n",
    "gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    type='messages',\n",
    "    title=\"Multi-Model Chat\",\n",
    "    description=\"Chat with me‚ù§Ô∏è\",\n",
    "    additional_inputs=[\n",
    "        gr.Dropdown(\n",
    "            choices=[\"GPT-4o-mini\", \"Gemini 2.0 Flash\"],\n",
    "            value=\"GPT-4o-mini\",\n",
    "            label=\"Select Model\"\n",
    "        )\n",
    "    ]\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12de4d20-da41-4a33-8c76-5497950eb8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_model(message,model):\n",
    "    if model ==\"GPT\":\n",
    "        result= chat_gpt(message)\n",
    "    elif model ==\"gemini\":\n",
    "        result =chat_gemini(message)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d1f05-ce5c-48f7-9243-5dae1ef36f54",
   "metadata": {},
   "source": [
    "### Gradio integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29ccc9c8-b87a-421c-a3fd-cd51ac27b1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7874\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat,type='messages').launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc97cf2-c7a9-44a9-8717-a1498b1874a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e9bdb-e6e8-4299-ac0d-4ca2726aedbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
